# eecs545-homework-1-solved
**TO GET THIS SOLUTION VISIT:** [EECS545 Homework 1 Solved](https://www.ankitcodinghub.com/product/eecs545-homework-1-solved/)


---

ğŸ“© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
ğŸ“± **WhatsApp:** +1 419 877 7882  
ğŸ“„ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;96598&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;0&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;0&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;0\/5 - (0 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;EECS545 Homework 1 Solved&quot;,&quot;width&quot;:&quot;0&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 0px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            <span class="kksr-muted">Rate this product</span>
    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column">
1 [Linear regression on a polynomial

The files q1xTrain.npy, q1xTest.npy, q1yTrain.npy and q1yTest.npy specify a linear regression problem for a polynomial. q1xTrain.npy represent the inputs (x(i) âˆˆ R) and q1yTrain.npy represents the outputs (y(i) âˆˆ R) of the training set, with one training example per row.

(a)&nbsp; You will compare the following two optimization methods, in finding the coefficients of a polynomial of degree one (i.e. slope and intercept) that minimize the training loss.

â€¢ Batch gradient descent

â€¢ Stochastic gradient descent

i.&nbsp; Give the coefficients generated by each of the optimization methods. Report the hy- perparameters used to generate the coefficients.

ii.&nbsp; We compare two optimization methods in terms of the number of epochs required for convergence. We define an â€œepochâ€ as one pass through the entire training samples. Compare the number of epochs to converge for the methods. Which method converges faster? Report the hyperparameters used for comparison. [Hint: in this question, the training process can be viewed as convergent when the mean squared error EMS = N1 ô°…Ni=1(w0 + w1x(i) âˆ’ y(i))2 on the training dataset is consistently small enough (e.g. â‰¤ 0.2).]

(b)&nbsp; Next, you will investigate the problem of over-fitting. Recall the figure from lecture that explored over-fitting as a function of the degree of the polynomial M, where the Root-Mean-Square (RMS) Error is defined as

</div>
</div>
<div class="layoutArea">
<div class="column">
1

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="layoutArea">
<div class="column">
where

</div>
<div class="column">
ERMS = ô°†2E(wâˆ—)/N, Nï£«M ï£¶2N

1ô°€ ô°€ (i) (i) 1ô°€ô° T (i) (i)ô°‚2 E(w)=2 ï£­ wjÏ†j(x)âˆ’yï£¸=2 wÏ†(x)âˆ’y :

i=1 j=0 i=1

</div>
</div>
<div class="layoutArea">
<div class="column">
i. Regenerate the above chart with the provided data. To find the parameters, use the closed form solution of linear regression (assuming all the condition is met) that minimize the error for a M -degree polynomial (for M = 0 . . . 9) for the training data in q1xTrain.npy and q1yTrain.npy. For the test curve, use the data in q1xTest.npy and q1yTest.npy. [Note: For different values of M, we assume the feature vector is Ï†(x(i)) = (1, x(i), (x(i))2, Â· Â· Â· , (x(i))M ). The trend of your curve is not necessarily the same as the sample plot.]

ii.&nbsp; Which degree polynomial would you say best fits the data? Was there evidence of under/over-fitting the data? Use your generated charts to justify your answer.

(c)&nbsp; Finally, you will explore the role of regularization. Recall the image from lecture that explored the effect of the regularization factor Î»:

i. Derive the closed form solution of the ridge regression, find the coefficients that minimize the error for a ninth degree polynomial (M = 9) given regularization factor Î» (for Î» âˆˆ {0, 10âˆ’8, 10âˆ’7, 10âˆ’6, 10âˆ’5, . . . , 10âˆ’1, 100(= 1)}) for the training data specified in q1xTrain.npy and q1yTrain.npy. Now use these parameters to plot the ERMS of both the training data and test data as a function of Î» and regenerate the above chart, using q1xTest.npy and q1yTest.npy as the test data). Specifically, use the following regularized objective function:

2

[Note: the trend of your curve is not necessarily the same as the sample chart.] ii. [2 points] Which Î» value seemed to work the best?

</div>
</div>
<div class="layoutArea">
<div class="column">
1 ô°€N Î»

</div>
</div>
<div class="layoutArea">
<div class="column">
(wT Ï†(x(i)) âˆ’ y(i))2 + 2 ||w||2.

for optimizing the parameters w, but please use the original (unregularized) ERMS for plotting.

</div>
</div>
<div class="layoutArea">
<div class="column">
i=1

</div>
</div>
<div class="layoutArea">
<div class="column">
2

</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="section">
<div class="layoutArea">
<div class="column">
2 [36 points] Locally weighted linear regression

Consider a linear regression problem in which we want to weight different training examples differently. Specifically, suppose we want to minimize

1 ô°€N

ED(w) = 2

where r(i) is the weight for the sample (x(i), y(i)). In class, we worked out what happens for the case where all the weights (the r(i)â€™s) are the same. In this problem, we will generalize some of those ideas to the weighted setting, and also implement the locally weighted linear regression algorithm. [Note: the weight r(i) can be different for each of the training example.]

(a) [2 points] Show that ED(w) can also be written as

ED(w) = (Xw âˆ’ y)T R(Xw âˆ’ y)

for an appropriate diagonal matrix R, and where X is a matrix whose i-th row is x(i) and y is the vector whose i-th entry is y(i). State clearly what R is.

(b) [8 points] If all the r(i)â€™s equal 1, then we saw in class that the normal equation is XT Xw = XT y,

and that the value of wâˆ— that minimizes ED(w) is given by (XT X)âˆ’1XT y. By finding the derivative âˆ‡wED(w) and setting that to zero, generalize the normal equation and the closed form solution to this weighted setting, and give the new value of wâˆ— that minimizes ED(w) in closed form as a function of X, R and y.

(c) [8 points] Suppose we have a training set {(x(i),y(i));i = 1â€¦,N} of N independent examples, but in which the y(i)â€™s were observed with differing variances. Specifically, suppose that

(i) (i) 1 ô°ƒ (y(i)âˆ’wTx(i))2ô°„ p(y |x ; w) = âˆš2Ï€Ïƒ(i) exp âˆ’ 2(Ïƒ(i))2

</div>
</div>
<div class="layoutArea">
<div class="column">
i=1

</div>
</div>
<div class="layoutArea">
<div class="column">
r(i)(wT x(i) âˆ’ y(i))2.

</div>
</div>
<div class="layoutArea">
<div class="column">
3

</div>
</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="layoutArea">
<div class="column">
I.e., y(i) has mean wT x(i) and variance (Ïƒ(i))2 (where the Ïƒ(i)â€™s are fixed, known, constants). Show that finding the maximum likelihood estimate of w reduces to solving a weighted linear regression problem. State clearly what the r(i)â€™s are in terms of the Ïƒ(i)â€™s.

(d) [18 points] The following items will use the files q2x.npy which contains the inputs (x(i)) and q2y.npy which contains the outputs (y(i)) for a linear regression problem, with one training example per row.

</div>
</div>
<div class="layoutArea">
<div class="column">
3

</div>
<div class="column">
<ol>
<li>[4 points] Implement (unweighted) linear regression (y = wT x) on this dataset (using the closed form solution we learned in lectures, remember to include the intercept term.). Plot on the same figure the data (each data sample can be shown as a point (x(i),y(i)) in the figure) and the straight line resulting from your fit.</li>
<li>[8 points] Implement locally weighted linear regression on this dataset (using the weighted normal equations you derived in part (b)), and plot on the same figure the data and the curve resulting from your fit. When evaluating local regression at a query point x (which is real-valued in this problem), use weights</li>
</ol>
(i) ô°ƒ (x âˆ’ x(i))2 ô°„ r =exp âˆ’ 2Ï„2

with a bandwidth parameter Ï„ = 0.8. (Again, remember to include the intercept term.)

iii. [6 points] Repeat (ii) four times with Ï„ = 0.1, 0.3, 2 and 10. Comment briefly on what happens

to the fit when Ï„ is too small or too large.

[22 points] Derivation and Proof

</div>
</div>
<div class="layoutArea">
<div class="column">
(a) [8 points] Consider the linear regression problem for 1D data, where we would like to learn a function h(x) = w1x + w0 with parameters w0 and w1 to minimize the sum squared error L = 1 ô°…N (y(i) âˆ’

2 i=1

h(x(i)))2 for N pairs of data samples (x(i),y(i)). Derive the solution for w0 and w1 for this 1D case of

Ì„ Ì„ N1 ô°…Ni=1x(i)y(i)âˆ’Y Ì„X Ì„ linear regression. Show the derivation to get the solution w0 = Y âˆ’ w1 X and w1 = 1 ô°…N x(i) 2 âˆ’X Ì„ 2

N i=1 where X Ì„ is the mean of {x(1),x(2),Â·Â·Â· ,x(N)} and Y Ì„ is the mean of {y(1),y(2),Â·Â·Â· ,y(N)}.

(b) [14 points] Consider the definition and property of positive (semi-)definite matrix. Let A be a real, symmetric d Ã— d matrix. A is positive semi-definite (PSD) if, for all z âˆˆ Rd, zT Az â‰¥ 0. A is positive definite(PD)if,forallzÌ¸=0,zTAz&gt;0. WewriteAâª°0whenAisPSD,andAâ‰»0when A is PD. The spectral theorem says that every real symmetric matrix A can be expressed via the spectral decomposition A = UÎ›UT where U is a d Ã— d matrix such that UUT = UT U = I and Î› = diag(Î»1,Î»2,Â·Â·Â· ,Î»d). Multiplying on the right by U we see that AU = UÎ›. If we let ui denote the i-th column of U, we have Aui = Î»iui for each i. Then Î»i are eigenvalues of A, and the corresponding columns are eigenvectors associated to Î»i. The eigenvalues constitute the â€œspectrumâ€ of A, and the spectral decomposition is also called the eigenvalue decomposition of A.

<ol>
<li>[6points]ProveAisPDiffÎ»i &gt;0foreachi.</li>
<li>[8 points] Consider the linear regression problem where Î¦ and y are as defined in class and the closed form solution is (Î¦T Î¦)âˆ’1Î¦T y. We can get the eigenvalues of symmetric matrix Î¦T Î¦ using spectral decomposition. We apply ridge regression, and the symmetric matrix in the solution is Î¦T Î¦ + Î²I. Prove that the ridge regression has an effect of shifting all singular values by a constant Î². For any Î² &gt; 0, ridge regression makes the matrix Î¦T Î¦ + Î²I PD.</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
&nbsp;

</div>
</div>
</div>
